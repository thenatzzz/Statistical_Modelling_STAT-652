(mean.cv = sqrt(apply(gb.cv, 1, mean)))
min.cv = apply(gb.cv, 2, min)
boxplot(gb.cv, use.cols=FALSE, las=2)
# x11(h=7,w=10,pointsize=8)
boxplot(sqrt(gb.cv), use.cols=FALSE, las=2)
parms = expand.grid(shr,dep)
row.names(gb.cv) = paste(parms[,2], parms[,1], sep="|")
row.names(opt.tree) = paste(parms[,2], parms[,1], sep="|")
opt.tree
gb.cv
(mean.tree = apply(opt.tree, 1, mean))
(mean.cv = sqrt(apply(gb.cv, 1, mean)))
(mean.tree = apply(opt.tree, 1, mean))
(mean.cv = sqrt(apply(gb.cv, 1, mean)))
opt.tree
gb.cv
gb.cv
# Variable Importance
# x11(h=7, w=6)
summary(pro.opt)
pro.opt <- gbm(data=prostate, lpsa~., distribution="gaussian",
n.trees=500, interaction.depth=2, shrinkage=0.0075,
bag.fraction=0.8)
# Variable Importance
# x11(h=7, w=6)
summary(pro.opt)
setwd("D:/Coding/SFU_CA/STAT-652 STAT/project1")
#############################
rm(list=ls(all=TRUE))
library(gbm)
# prostate <-  read.table("Prostate.csv",
# header=TRUE, sep=",", na.strings=" ")
data <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
#############################
rm(list=ls(all=TRUE))
library(gbm)
# prostate <-  read.table("Prostate.csv",
# header=TRUE, sep=",", na.strings=" ")
prostate <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
#  Let's do R=2 reps of 5-fold CV.
set.seed(182674455)
V=5
R=2
n2 = nrow(prostate)
# Create the folds and save in a matrix
folds = matrix(NA, nrow=n2, ncol=R)
for(r in 1:R){
folds[,r]=floor((sample.int(n2)-1)*V/n2) + 1
}
shr = c(.001,.005,.025,.125)
dep = c(2,4,6)
### Second grid
#dep = c(1,2,3,4)
#shr = c(0.0025, 0.005, 0.0075, 0.01)
trees = 10000
NS = length(shr)
ND = length(dep)
gb.cv = matrix(NA, nrow=ND*NS, ncol=V*R)
opt.tree = matrix(NA, nrow=ND*NS, ncol=V*R)
qq = 1
for(r in 1:R){
for(v in 1:V){
pro.train = prostate[folds[,r]!=v,]
pro.test = prostate[folds[,r]==v,]
counter=1
for(d in dep){
for(s in shr){
pro.gbm <- gbm(data=pro.train, lpsa~., distribution="gaussian",
n.trees=trees, interaction.depth=d, shrinkage=s,
bag.fraction=0.8)
treenum = min(trees, 2*gbm.perf(pro.gbm, method="OOB", plot.it=FALSE))
opt.tree[counter,qq] = treenum
preds = predict(pro.gbm, newdata=pro.test, n.trees=treenum)
gb.cv[counter,qq] = mean((preds - pro.test$lpsa)^2)
counter=counter+1
}
}
qq = qq+1
}
}
#############################
rm(list=ls(all=TRUE))
library(gbm)
# prostate <-  read.table("Prostate.csv",
# header=TRUE, sep=",", na.strings=" ")
prostate <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
#  Let's do R=2 reps of 5-fold CV.
set.seed(182674455)
V=5
R=2
n2 = nrow(prostate)
# Create the folds and save in a matrix
folds = matrix(NA, nrow=n2, ncol=R)
for(r in 1:R){
folds[,r]=floor((sample.int(n2)-1)*V/n2) + 1
}
shr = c(.001,.005,.025,.125)
dep = c(2,4,6)
### Second grid
#dep = c(1,2,3,4)
#shr = c(0.0025, 0.005, 0.0075, 0.01)
trees = 10000
NS = length(shr)
ND = length(dep)
gb.cv = matrix(NA, nrow=ND*NS, ncol=V*R)
opt.tree = matrix(NA, nrow=ND*NS, ncol=V*R)
qq = 1
for(r in 1:R){
for(v in 1:V){
pro.train = prostate[folds[,r]!=v,]
pro.test = prostate[folds[,r]==v,]
counter=1
for(d in dep){
for(s in shr){
pro.gbm <- gbm(data=pro.train, Y~., distribution="gaussian",
n.trees=trees, interaction.depth=d, shrinkage=s,
bag.fraction=0.8)
treenum = min(trees, 2*gbm.perf(pro.gbm, method="OOB", plot.it=FALSE))
opt.tree[counter,qq] = treenum
preds = predict(pro.gbm, newdata=pro.test, n.trees=treenum)
gb.cv[counter,qq] = mean((preds - pro.test$lpsa)^2)
counter=counter+1
}
}
qq = qq+1
}
}
parms = expand.grid(shr,dep)
row.names(gb.cv) = paste(parms[,2], parms[,1], sep="|")
row.names(opt.tree) = paste(parms[,2], parms[,1], sep="|")
opt.tree
gb.cv
parms = expand.grid(shr,dep)
row.names(gb.cv) = paste(parms[,2], parms[,1], sep="|")
row.names(opt.tree) = paste(parms[,2], parms[,1], sep="|")
opt.tree
gb.cv
(mean.tree = apply(opt.tree, 1, mean))
(mean.cv = sqrt(apply(gb.cv, 1, mean)))
min.cv = apply(gb.cv, 2, min)
# boxplot(sqrt(gb.cv), use.cols=FALSE, las=2)
boxplot(gb.cv, use.cols=FALSE, las=2)
#############################
rm(list=ls(all=TRUE))
library(gbm)
# prostate <-  read.table("Prostate.csv",
# header=TRUE, sep=",", na.strings=" ")
prostate <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
#  Let's do R=2 reps of 5-fold CV.
set.seed(182674455)
V=5
R=2
n2 = nrow(prostate)
# Create the folds and save in a matrix
folds = matrix(NA, nrow=n2, ncol=R)
for(r in 1:R){
folds[,r]=floor((sample.int(n2)-1)*V/n2) + 1
}
shr = c(.001,.005,.025,.125)
dep = c(2,4,6)
### Second grid
#dep = c(1,2,3,4)
#shr = c(0.0025, 0.005, 0.0075, 0.01)
trees = 10000
NS = length(shr)
ND = length(dep)
gb.cv = matrix(NA, nrow=ND*NS, ncol=V*R)
opt.tree = matrix(NA, nrow=ND*NS, ncol=V*R)
qq = 1
for(r in 1:R){
for(v in 1:V){
pro.train = prostate[folds[,r]!=v,]
pro.test = prostate[folds[,r]==v,]
counter=1
for(d in dep){
for(s in shr){
pro.gbm <- gbm(data=pro.train, Y~., distribution="gaussian",
n.trees=trees, interaction.depth=d, shrinkage=s,
bag.fraction=0.8)
treenum = min(trees, 2*gbm.perf(pro.gbm, method="OOB", plot.it=FALSE))
opt.tree[counter,qq] = treenum
preds = predict(pro.gbm, newdata=pro.test, n.trees=treenum)
gb.cv[counter,qq] = mean((preds - pro.test$Y)^2)
counter=counter+1
}
}
qq = qq+1
}
}
parms = expand.grid(shr,dep)
row.names(gb.cv) = paste(parms[,2], parms[,1], sep="|")
row.names(opt.tree) = paste(parms[,2], parms[,1], sep="|")
opt.tree
gb.cv
gb.cv
(mean.tree = apply(opt.tree, 1, mean))
(sqrt.mean.cv = sqrt(apply(gb.cv, 1, mean)))
(mean.cv = apply(gb.cv, 1, mean))
# boxplot(sqrt(gb.cv), use.cols=FALSE, las=2)
boxplot(gb.cv, use.cols=FALSE, las=2)
boxplot(sqrt(gb.cv), use.cols=FALSE, las=2)
# boxplot(sqrt(t(gb.cv)/min.cv), use.cols=TRUE, las=2,
# main="GBM Fine-Tuning Variables and Node Sizes")
boxplot(t(gb.cv)/min.cv, use.cols=TRUE, las=2,
main="GBM Fine-Tuning Variables and Node Sizes")
min.cv = apply(gb.cv, 2, min)
# boxplot(sqrt(t(gb.cv)/min.cv), use.cols=TRUE, las=2,
# main="GBM Fine-Tuning Variables and Node Sizes")
boxplot(t(gb.cv)/min.cv, use.cols=TRUE, las=2,
main="GBM Fine-Tuning Variables and Node Sizes")
pro.opt <- gbm(data=prostate, Y~., distribution="gaussian",
n.trees=500, interaction.depth=6, shrinkage=0.025,
bag.fraction=0.8)
# Variable Importance
# x11(h=7, w=6)
summary(pro.opt)
# clear var
rm(list=ls(all=TRUE))
# Clear plots
dev.off()  # But only if there IS a plot
set.seed(1234,kind="Mersenne-Twister")
################## read data ###################
data <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
dim(data)
head(data)
# shuffle data
rows <- sample(nrow(data))
data <- data[rows, ]
head(data)
source("Helper Functions.R")
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(h2o)          # an extremely fast java-based platform
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
data_split <- initial_split(data, prop = 0.8)
data_train <- training(data_split)
data_test  <- testing(data_split)
gbm.fit.final <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 213,
interaction.depth = 3,
shrinkage = 0.1,
n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
RF <- randomForest(
y         = data_train$Y,
x            = data_train[,2:16],
num.trees       = 300,
mtry            = 5,
min.node.size   = 9,
sample.fraction = .55,
importance=TRUE
)
####### TEST (prediction) #########
### prediction ###
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, data_test)
pred2 <- predict(RF, n.trees = gbm.fit.final$n.trees, data_test)
#
# caret::RMSE(pred, data_test$Y)
MSPE = get.MSPE(data_test$Y, pred) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred2) # Our helper function
MSPE
# clear var
rm(list=ls(all=TRUE))
# Clear plots
dev.off()  # But only if there IS a plot
set.seed(1234,kind="Mersenne-Twister")
################## read data ###################
data <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
dim(data)
head(data)
# shuffle data
rows <- sample(nrow(data))
data <- data[rows, ]
head(data)
source("Helper Functions.R")
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(h2o)          # an extremely fast java-based platform
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
data_split <- initial_split(data, prop = 0.8)
data_train <- training(data_split)
data_test  <- testing(data_split)
gbm.fit.final <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 213,
interaction.depth = 3,
shrinkage = 0.1,
n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
RF <- randomForest(
y         = data_train$Y,
x            = data_train[,2:16],
num.trees       = 300,
mtry            = 5,
min.node.size   = 9,
sample.fraction = .55,
importance=TRUE
)
gbm.fit.final_2 <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 500,
interaction.depth = 6,
shrinkage = 0.025,
# n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
####### TEST (prediction) #########
### prediction ###
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, data_test)
pred2 <- predict(RF, n.trees = RF$n.trees, data_test)
pred3 <- predict(gbm.fit.final_2, n.trees = gbm.fit.final_2$n.trees, data_test)
#
# caret::RMSE(pred, data_test$Y)
MSPE = get.MSPE(data_test$Y, pred) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred2) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred3) # Our helper function
MSPE
# clear var
rm(list=ls(all=TRUE))
# Clear plots
dev.off()  # But only if there IS a plot
################## read data ###################
data <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
dim(data)
head(data)
# shuffle data
rows <- sample(nrow(data))
data <- data[rows, ]
head(data)
source("Helper Functions.R")
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(h2o)          # an extremely fast java-based platform
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
data_split <- initial_split(data, prop = 0.8)
data_train <- training(data_split)
data_test  <- testing(data_split)
gbm.fit.final <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 213,
interaction.depth = 3,
shrinkage = 0.1,
n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
RF <- randomForest(
y         = data_train$Y,
x            = data_train[,2:16],
num.trees       = 300,
mtry            = 5,
min.node.size   = 9,
sample.fraction = .55,
importance=TRUE
)
gbm.fit.final_2 <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 500,
interaction.depth = 6,
shrinkage = 0.025,
# n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
####### TEST (prediction) #########
### prediction ###
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, data_test)
pred2 <- predict(RF, n.trees = RF$n.trees, data_test)
pred3 <- predict(gbm.fit.final_2, n.trees = gbm.fit.final_2$n.trees, data_test)
#
# caret::RMSE(pred, data_test$Y)
MSPE = get.MSPE(data_test$Y, pred) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred2) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred3) # Our helper function
MSPE
# clear var
rm(list=ls(all=TRUE))
# Clear plots
dev.off()  # But only if there IS a plot
set.seed(1234,kind="Mersenne-Twister")
################## read data ###################
data <-  read.table("Data2020.csv",
header=TRUE, sep=",", na.strings=" ")
dim(data)
head(data)
# shuffle data
rows <- sample(nrow(data))
data <- data[rows, ]
head(data)
source("Helper Functions.R")
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(h2o)          # an extremely fast java-based platform
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
data_split <- initial_split(data, prop = 0.8)
data_train <- training(data_split)
data_test  <- testing(data_split)
gbm.fit.final <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 213,
interaction.depth = 3,
shrinkage = 0.1,
n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
RF <- randomForest(
y         = data_train$Y,
x            = data_train[,2:16],
num.trees       = 300,
mtry            = 5,
min.node.size   = 9,
sample.fraction = .55,
importance=TRUE
)
gbm.fit.final_2 <- gbm(
formula = Y ~ .,
distribution = "gaussian",
data = data_train,
n.trees = 500,
interaction.depth = 6,
shrinkage = 0.025,
# n.minobsinnode = 15,
bag.fraction = 0.8,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
####### TEST (prediction) #########
### prediction ###
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, data_test)
pred2 <- predict(RF, n.trees = RF$n.trees, data_test)
pred3 <- predict(gbm.fit.final_2, n.trees = gbm.fit.final_2$n.trees, data_test)
#
# caret::RMSE(pred, data_test$Y)
MSPE = get.MSPE(data_test$Y, pred) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred2) # Our helper function
MSPE
MSPE = get.MSPE(data_test$Y, pred3) # Our helper function
MSPE
# read data for prediction
data_test <-  read.table("Data2020testX.csv",
header=TRUE, sep=",", na.strings=" ")
gbm.fit.final_2$n.trees
# read data for prediction
data_test <-  read.table("Data2020testX.csv",
header=TRUE, sep=",", na.strings=" ")
predictions=predict(gbm.fit.final_2, data_test, n.trees = gbm.fit.final_2$n.trees)
# predictions=predict(RF, data_test)
predictions
predictions=predict(gbm.fit.final, data_test, n.trees = gbm.fit.final$n.trees)
# predictions=predict(RF, data_test)
predictions
