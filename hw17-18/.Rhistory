pred.log.glmnet = predict(fit.log.glmnet, X.valid.scale, type = "class",
s = 0)
table(Y.valid, pred.log.glmnet, dnn = c("Observed", "Predicted"))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet))
### While we're looking at the glmnet package, let's do LASSO. We need to
### choose lambda using CV. Fortunately, the cv.glmnet() function does this
### for us. The syntax for cv.glmnet() is the same as for glmnet().
fit.CV.lasso = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
### Plotting the cv.glmnet() output shows us how CV misclassification
### rate changes with lambda (actually, it plots the multinomial deviance,
### which is a transformation of the misclassification rate).
plot(fit.CV.lasso)
### The CV-min and CV-1se lambda values are stored in the output from
### cv.glmnet()
lambda.min = fit.CV.lasso$lambda.min
lambda.1se = fit.CV.lasso$lambda.1se
### Let's check which predictors are included in each "best" model. We
### can get the coefficients using the coef() function, setting s to
### the appropriate lambda value.
coef(fit.CV.lasso, s = lambda.min)
coef(fit.CV.lasso, s = lambda.1se)
fit.CV.lasso
View(data)
##########################################
########## HW 18 ###############
#####################################
rm(list=ls(all=TRUE))
ve <-  read.csv("vehicle.csv")
ve$class = factor(ve$class, labels=c('2D','4D','BUS','VAN'))
# ve$class = as.factor(ve$class)
ve
set.seed(46685326,kind="Mersenne-Twister")
perm <- sample(x=nrow(ve))
set1 <- ve[which(perm <= 3*nrow(ve)/4),]
set2 <- ve[which(perm >  3*nrow(ve)/4),]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
############## 1.
##### 1.a
set1.rescale <- data.frame(cbind(rescale(set1[,-19], set1[,-19]), class=set1$class))
set2.rescale <- data.frame(cbind(rescale(set2[,-19], set1[,-19]), class=set2$class))
summary(set1.rescale)
summary(set2.rescale)
# str(set1.rescale)
library(nnet)
###### 1.b
mod.fit <- multinom(data=set1.rescale, formula=class ~ .,
trace=TRUE, maxit = 1500)
summary(mod.fit)
# Adding a function to perform LR Tests.  Legitimate here since I am fitting one model
library(car)
Anova(mod.fit)
# Misclassification Errors
pred.class.1 <- predict(mod.fit, newdata=set1.rescale)
# type="class")
pred.class.2 <- predict(mod.fit, newdata=set2.rescale)
# type="class")
(mul.misclass.train <- mean(ifelse(pred.class.1 == set1$class,
yes=0, no=1)))
(mul.misclass.test <- mean(ifelse(pred.class.2 == set2$class,
yes=0, no=1)))
# Estimated probabilities for test set
pred.probs.2 <- predict(mod.fit, newdata=set2.rescale,
type="probs")
round(head(pred.probs.2),3)
# Test set confusion matrix
# table(set2$class, pred.class.2, dnn=c("Obs","Pred"))
table(pred.class.2,set2$class, dnn=c("Predicted","Observed"))
# Number of parameters estimated (just FYI)
mod.fit$edf
###### 2.
library(glmnet)
logit.fit <- glmnet(x=as.matrix(set1.rescale[,-19]),
y=set1.rescale[,19], family="multinomial")
# "Optimal" LASSO Fit
logit.cv <- cv.glmnet(x=as.matrix(set1.rescale[,1:18]),
y=set1.rescale[,19], family="multinomial")
logit.cv
# x11()
plot(logit.cv)
## Find nonzero lasso coefficients
c <- coef(logit.fit,s=logit.cv$lambda.min)
c <- coef(logit.cv,s=logit.cv$lambda.min)
c
coef(logit.cv, s = logit.cv$lambda.min)
##########################################
########## HW 18 ###############
#####################################
rm(list=ls(all=TRUE))
### Read-in and process the data
source("Read Wine Data - Class.R")
### Activate packages
library(nnet)   # For logistic regression
library(car)    # For ANOVA after logistic regression with nnet
library(glmnet) # For logistic regression and LASSO
library(MASS)   # For discriminant analysis
### Set random seed, using Mersenne-Twister for compatibility.
set.seed(46536737, kind = "Mersenne-Twister")
### Split the data into training and validation sets
p.train = 0.75
n = nrow(data)
n.train = floor(p.train*n)
ind.random = sample(1:n)
data.train = data[ind.random <= n.train,]
data.valid = data[ind.random > n.train,]
Y.valid = data.valid[,1]
### Rescale the columns of x1 so that the columns of x2 fall between 0 and 1
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
### Create copies of our datasets and rescale
data.train.scale = data.train
data.valid.scale = data.valid
data.train.scale[,-1] = rescale(data.train.scale[,-1], data.train[,-1])
data.valid.scale[,-1] = rescale(data.valid.scale[,-1], data.train[,-1])
### Fit a logistic regression model using the multinom() function from the
### nnet package. This function uses formula/data frame syntax. We can
### optionally specify the maximum number of iterations multinom is
### allowed to use to fit the model using the maxit input. The defult is 100,
### and this is usually adequate.
fit.log.nnet = multinom(quality ~ ., data = data.train.scale, maxit = 200)
### We can get information about our fitted model using the summary()
### function.
summary(fit.log.nnet)
### We can do significance tests for the coefficients in our model using
### the Anova() function from the car package.
### Note: The Anova() function reports Type II hypothesis tests. These are
### tests for a variable's effect in a model already containing the
### other predictors. Type I tests are sequential (i.e. first vs nothing,
### first and second vs only first, etc.).
Anova(fit.log.nnet)
### Let's plot the two most significant predictors, using color to
### distinguish quality. To get the color for each level of quality,
### we will use the ifelse() function. See this tutorial's video for
### details of the syntax.
col.qual = ifelse(data.train.scale[,1] == "low", "Red",
ifelse(data.train.scale[,1] == "med", "Blue", "Green"))
plot(data.train$sulphates, data.train$alcohol, col = col.qual,
xlab = "Sulphates", ylab = "Alcohol",
main = "Wine Quality by Alcohol and Sulphates")
legend(x = "topright", legend = c("Low", "Medium", "High"),
col = c("Red", "Blue", "Green"), pch = 1)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet = predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet,        ### Confusion matrix
dnn = c("Observed", "Predicted"))
(misclass.log.nnet = mean(pred.log.nnet != Y.valid)) ### Misclass rate
### The glmnet() function uses predictor matrix/response vector syntax,
### so we need to extract these from our training and validation sets.
### We also have to convert the predictors to a matrix using the
### as.matrix() function.
X.train.scale = as.matrix(data.train.scale[,-1])
Y.train = data.train.scale[,1]
X.valid.scale = as.matrix(data.valid.scale[,-1])
Y.valid = data.valid.scale[,1]
### The glmnet() function in the glmnet() package can be used to fit
### logistic regression models. The syntax is the same as for regression,
### but now we have to set family="multinomial". This approach actually
### fits a LASSO version, but we can get ordinary logistic regression by
### setting s=0 when doing prediction (they use s instead of lambda when
### doing prediction...sometimes R is weird).
### Note: The code will allow you to specify lambda when fitting your model.
### DON'T DO THIS. Counterintuitively, it is actually faster and more
### stable to fit models for a whole sequence of lambda values than to
### fit a single LASSO model.
fit.log.glmnet = glmnet(X.train.scale, Y.train, family = "multinomial")
### Get predictions and investigate performance. The predict() function
### for glmnet() can give several different types of output. To get
### predicted values, set type="class". Remember to set s=0 for logistic
### regression.
pred.log.glmnet = predict(fit.log.glmnet, X.valid.scale, type = "class",
s = 0)
table(Y.valid, pred.log.glmnet, dnn = c("Observed", "Predicted"))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet))
### While we're looking at the glmnet package, let's do LASSO. We need to
### choose lambda using CV. Fortunately, the cv.glmnet() function does this
### for us. The syntax for cv.glmnet() is the same as for glmnet().
fit.CV.lasso = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
### Plotting the cv.glmnet() output shows us how CV misclassification
### rate changes with lambda (actually, it plots the multinomial deviance,
### which is a transformation of the misclassification rate).
plot(fit.CV.lasso)
### The CV-min and CV-1se lambda values are stored in the output from
### cv.glmnet()
lambda.min = fit.CV.lasso$lambda.min
lambda.1se = fit.CV.lasso$lambda.1se
### Let's check which predictors are included in each "best" model. We
### can get the coefficients using the coef() function, setting s to
### the appropriate lambda value.
coef(fit.CV.lasso, s = lambda.min)
##########################################
########## HW 18 ###############
#####################################
rm(list=ls(all=TRUE))
ve <-  read.csv("vehicle.csv")
ve$class = factor(ve$class, labels=c('2D','4D','BUS','VAN'))
# ve$class = as.factor(ve$class)
ve
set.seed(46685326,kind="Mersenne-Twister")
perm <- sample(x=nrow(ve))
set1 <- ve[which(perm <= 3*nrow(ve)/4),]
set2 <- ve[which(perm >  3*nrow(ve)/4),]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
############## 1.
##### 1.a
set1.rescale <- data.frame(cbind(rescale(set1[,-19], set1[,-19]), class=set1$class))
set2.rescale <- data.frame(cbind(rescale(set2[,-19], set1[,-19]), class=set2$class))
summary(set1.rescale)
summary(set2.rescale)
# str(set1.rescale)
library(nnet)
###### 1.b
mod.fit <- multinom(data=set1.rescale, formula=class ~ .,
trace=TRUE, maxit = 1500)
summary(mod.fit)
# Adding a function to perform LR Tests.  Legitimate here since I am fitting one model
library(car)
Anova(mod.fit)
# Misclassification Errors
pred.class.1 <- predict(mod.fit, newdata=set1.rescale)
# type="class")
pred.class.2 <- predict(mod.fit, newdata=set2.rescale)
# type="class")
(mul.misclass.train <- mean(ifelse(pred.class.1 == set1$class,
yes=0, no=1)))
(mul.misclass.test <- mean(ifelse(pred.class.2 == set2$class,
yes=0, no=1)))
# Estimated probabilities for test set
pred.probs.2 <- predict(mod.fit, newdata=set2.rescale,
type="probs")
round(head(pred.probs.2),3)
# Test set confusion matrix
# table(set2$class, pred.class.2, dnn=c("Obs","Pred"))
table(pred.class.2,set2$class, dnn=c("Predicted","Observed"))
# Number of parameters estimated (just FYI)
mod.fit$edf
###### 2.
library(glmnet)
logit.fit <- glmnet(x=as.matrix(set1.rescale[,-19]),
y=set1.rescale[,19], family="multinomial")
# "Optimal" LASSO Fit
logit.cv <- cv.glmnet(x=as.matrix(set1.rescale[,1:18]),
y=set1.rescale[,19], family="multinomial")
logit.cv
# x11()
plot(logit.cv)
## Find nonzero lasso coefficients
# c <- coef(logit.fit,s=logit.cv$lambda.min)
c <- coef(logit.cv,s=logit.cv$lambda.min)
cmat <- cbind(as.matrix(c[[1]]), as.matrix(c[[2]]),
as.matrix(c[[3]]),as.matrix(c[[4]]))
round(cmat,2)
colnames(cmat) = c('2D','4D','BUS','VAN')
# round(cmat,2)
colnames(cmat) = c('2D','4D','BUS','VAN')
round(cmat,2)
cmat!=0
lascv.pred.train <- predict(object=logit.cv, type="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:6]))
lascv.pred.train <- predict(object=logit.cv, class="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:18]))
lascv.pred.test <- predict(logit.cv, type="class",
s=logit.cv$lambda.min,
newx=as.matrix(set2.rescale[,1:18]))
lascv.pred.train <- predict(object=logit.cv, class="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:18]))
lascv.pred.test <- predict(logit.cv, class="class",
s=logit.cv$lambda.min,
newx=as.matrix(set2.rescale[,1:18]))
(lascvmisclass.train <-
mean(ifelse(lascv.pred.train == set1$type, yes=0, no=1)))
(lascvmisclass.test <-
mean(ifelse(lascv.pred.test == set2$type, yes=0, no=1)))
##########################################
########## HW 18 ###############
#####################################
rm(list=ls(all=TRUE))
ve <-  read.csv("vehicle.csv")
ve$class = factor(ve$class, labels=c('2D','4D','BUS','VAN'))
# ve$class = as.factor(ve$class)
ve
set.seed(46685326,kind="Mersenne-Twister")
perm <- sample(x=nrow(ve))
set1 <- ve[which(perm <= 3*nrow(ve)/4),]
set2 <- ve[which(perm >  3*nrow(ve)/4),]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
############## 1.
##### 1.a
set1.rescale <- data.frame(cbind(rescale(set1[,-19], set1[,-19]), class=set1$class))
set2.rescale <- data.frame(cbind(rescale(set2[,-19], set1[,-19]), class=set2$class))
summary(set1.rescale)
summary(set2.rescale)
# str(set1.rescale)
library(nnet)
###### 1.b
mod.fit <- multinom(data=set1.rescale, formula=class ~ .,
trace=TRUE, maxit = 1500)
summary(mod.fit)
# Adding a function to perform LR Tests.  Legitimate here since I am fitting one model
library(car)
Anova(mod.fit)
# Misclassification Errors
pred.class.1 <- predict(mod.fit, newdata=set1.rescale)
# type="class")
pred.class.2 <- predict(mod.fit, newdata=set2.rescale)
# type="class")
(mul.misclass.train <- mean(ifelse(pred.class.1 == set1$class,
yes=0, no=1)))
(mul.misclass.test <- mean(ifelse(pred.class.2 == set2$class,
yes=0, no=1)))
# Estimated probabilities for test set
pred.probs.2 <- predict(mod.fit, newdata=set2.rescale,
type="probs")
round(head(pred.probs.2),3)
# Test set confusion matrix
# table(set2$class, pred.class.2, dnn=c("Obs","Pred"))
table(pred.class.2,set2$class, dnn=c("Predicted","Observed"))
# Number of parameters estimated (just FYI)
mod.fit$edf
###### 2.
library(glmnet)
logit.fit <- glmnet(x=as.matrix(set1.rescale[,-19]),
y=set1.rescale[,19], family="multinomial")
# "Optimal" LASSO Fit
logit.cv <- cv.glmnet(x=as.matrix(set1.rescale[,1:18]),
y=set1.rescale[,19], family="multinomial")
logit.cv
# x11()
plot(logit.cv)
## Find nonzero lasso coefficients
# c <- coef(logit.fit,s=logit.cv$lambda.min)
c <- coef(logit.cv,s=logit.cv$lambda.min)
cmat <- cbind(as.matrix(c[[1]]), as.matrix(c[[2]]),
as.matrix(c[[3]]),as.matrix(c[[4]]))
# round(cmat,2)
colnames(cmat) = c('2D','4D','BUS','VAN')
round(cmat,2)
cmat!=0
lascv.pred.train <- predict(object=logit.cv, class="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:18]))
lascv.pred.test <- predict(logit.cv, class="class",
s=logit.cv$lambda.min,
newx=as.matrix(set2.rescale[,1:18]))
(lascvmisclass.train <-
mean(ifelse(lascv.pred.train == set1$type, yes=0, no=1)))
(lascvmisclass.test <-
mean(ifelse(lascv.pred.test == set2$type, yes=0, no=1)))
(lascvmisclass.train <-
mean(ifelse(lascv.pred.train == set1$class, yes=0, no=1)))
(lascvmisclass.test <-
mean(ifelse(lascv.pred.test == set2$class, yes=0, no=1)))
lascv.pred.train
set1.rescale[,1:18]
s=logit.cv$lambda.min
logit.cv$lambda.min
lascv.pred.train <- predict(object=logit.cv, type="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:18]))
(lascvmisclass.train <-
mean(ifelse(lascv.pred.train == set1$class, yes=0, no=1)))
lascv.pred.train <- predict(object=logit.cv, type="class",
s=logit.cv$lambda.min,
newx=as.matrix(set1.rescale[,1:18]))
lascv.pred.test <- predict(logit.cv, type="class",
s=logit.cv$lambda.min,
newx=as.matrix(set2.rescale[,1:18]))
(lascvmisclass.train <-
mean(ifelse(lascv.pred.train == set1$class, yes=0, no=1)))
(lascvmisclass.test <-
mean(ifelse(lascv.pred.test == set2$class, yes=0, no=1)))
View(set1)
#####################################################
##################### 3.
#### 3.a
###############################################################
library(MASS)
set1s <- apply(set1[,-19], 2, scale)
set1s <- data.frame(set1s,type=set1$class)
lda.fit.s <- lda(data=set1s, class~.)
set1s <- apply(set1[,-19], 2, scale)
set1s <- data.frame(set1s,class=set1$class)
lda.fit.s <- lda(data=set1s, class~.)
lda.fit.s
# Fit gives identical results as without scaling, but
#  can't interpret means
lda.fit <- lda(x=set1[,-19], grouping=set1$class)
lda.fit
# Plot results.  Create standard colours for classes.
class.col <-  ifelse(set1$class=="2D",y=53,n=
ifelse(set1$class=="4D",y=68,n=
ifelse(set1$class=='BUS',y=203,n=464)))
# x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="histogram")
# x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="histogram")
# x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="histogram")
# x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="histogram")
x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="histogram")
x11(h=7,w=6,pointsize=12)
plot(lda.fit, dimen=1, type="density")
x11(h=7,w=6,pointsize=12)
plot(lda.fit, col=colors()[class.col])
# Calculate in-sample and out-of-sample misclassification error
lda.pred.train <- predict(lda.fit, newdata=set1[,-19])$class
lda.pred.test <- predict(lda.fit, newdata=set2[,-19])$class
(lmisclass.train <- mean(ifelse(lda.pred.train == set1$class, yes=0, no=1)))
(lmisclass.test <- mean(ifelse(lda.pred.test == set2$class, yes=0, no=1)))
######## 4.
qda.fit <- qda(data=set1, class~.)
qda.fit
qda.pred.train <- predict(qda.fit, newdata=set1)$class
qda.pred.test <- predict(qda.fit, newdata=set2)$class
(qmisclass.train <- mean(ifelse(qda.pred.train == set1$class, yes=0, no=1)))
(qmisclass.test <- mean(ifelse(qda.pred.test == set2$class, yes=0, no=1)))
ve <-  read.csv("vehicle.csv")
ve$class = factor(ve$class, labels=c('2D','4D','BUS','VAN'))
# ve$class = as.factor(ve$class)
ve
set.seed(46685326,kind="Mersenne-Twister")
perm <- sample(x=nrow(ve))
set1 <- ve[which(perm <= 3*nrow(ve)/4),]
set2 <- ve[which(perm >  3*nrow(ve)/4),]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
############## 1.
##### 1.a
set1.rescale <- data.frame(cbind(rescale(set1[,-19], set1[,-19]), class=set1$class))
set2.rescale <- data.frame(cbind(rescale(set2[,-19], set1[,-19]), class=set2$class))
summary(set1.rescale)
summary(set2.rescale)
# str(set1.rescale)
library(nnet)
###### 1.b
mod.fit <- multinom(data=set1.rescale, formula=class ~ .,
trace=TRUE, maxit = 1500)
summary(mod.fit)
# Adding a function to perform LR Tests.  Legitimate here since I am fitting one model
library(car)
Anova(mod.fit)
##########################################
########## HW 18 ###############
#####################################
rm(list=ls(all=TRUE))
ve <-  read.csv("vehicle.csv")
ve$class = factor(ve$class, labels=c('2D','4D','BUS','VAN'))
# ve$class = as.factor(ve$class)
ve
set.seed(46685326,kind="Mersenne-Twister")
perm <- sample(x=nrow(ve))
set1 <- ve[which(perm <= 3*nrow(ve)/4),]
set2 <- ve[which(perm >  3*nrow(ve)/4),]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
############## 1.
##### 1.a
set1.rescale <- data.frame(cbind(rescale(set1[,-19], set1[,-19]), class=set1$class))
set2.rescale <- data.frame(cbind(rescale(set2[,-19], set1[,-19]), class=set2$class))
summary(set1.rescale)
summary(set2.rescale)
# str(set1.rescale)
library(nnet)
###### 1.b
mod.fit <- multinom(data=set1.rescale, formula=class ~ .,
trace=TRUE, maxit = 1500)
summary(mod.fit)
# Adding a function to perform LR Tests.  Legitimate here since I am fitting one model
library(car)
Anova(mod.fit)
